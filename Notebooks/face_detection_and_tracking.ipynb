{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e14883",
   "metadata": {},
   "source": [
    "# Exercise 4 -Computer Vision\n",
    "\n",
    "\n",
    "### 4.1 - Face Detection and Tracking\n",
    "In this task you will implement face detection and tracking using OpenCV. Specifically we are utilizing Cascade classifiers which implements Viola-Jones detection algorithm.\n",
    "\n",
    "**Reference**\n",
    "- [OpenCV documentation on cascade classifier](https://docs.opencv.org/master/db/d28/tutorial_cascade_classifier.html)\n",
    "\n",
    "### 4.1.1\n",
    "Execute the code below to initiate the cascadee classifier and the utility libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed45085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import time\n",
    "import imutils\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb82336",
   "metadata": {},
   "source": [
    "### 4.1.2\n",
    "\n",
    "Similar to Task 3, the first step is to obtain a frame from video file and pre-processing it. \n",
    "\n",
    "**Your task**\n",
    "\n",
    "Complete prep() function below which performs following using opencv and imutils libraries. The steps already implemented are marked with a tick \"âœ“\"\n",
    "\n",
    "- [x] Takes a frame from video feed as the input\n",
    "- [ ] Resize the frame while protecting the aspect ratio (width = 600) \n",
    "- [ ] Flip the image\n",
    "- [ ] Convert the frame to grayscale image\n",
    "- [x] Return grayscale image and resized image \n",
    "\n",
    "**References**\n",
    "\n",
    "- [imutils documentation](https://github.com/PyImageSearch/imutils#imutils)\n",
    "- [Fip an array with OpenCV](https://docs.opencv.org/4.x/d2/de8/group__core__array.html#gaca7be533e3dac7feb70fc60635adf441)\n",
    "- [color conversion with OpenCV](https://docs.opencv.org/4.x/d8/d01/group__imgproc__color__conversions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce9f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(img):\n",
    "    ## ToDo 4.1.2\n",
    "    #  1. resize using  imutils.resize()\n",
    "    img = imutils.resize(img, width=600)\n",
    "    #  2. flip image vertically using cv2.flip()\n",
    "    img =  cv2.flip(img, 1)\n",
    "    #  3. convert to gray color using cv2.cvtColor()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return gray, img    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96848eab",
   "metadata": {},
   "source": [
    "### 4.1.3\n",
    "\n",
    "In 4.1.1 we initialized an instance of cascade classifier. Tracking a face can be broken down into 3 steps as below\n",
    "\n",
    "1. Detect Faces and ROIs\n",
    "\n",
    "   The cascade classifier has a member function which can detect faces of multiple scales in a given image. The area where a face is detected becomes a region of interest (ROI) for extracting meaningful information. \n",
    "\n",
    "    **References** : \n",
    "    [Multiscale face detection member function of cascade classifier](https://docs.opencv.org/3.4/d1/de5/classcv_1_1CascadeClassifier.html#a90fe1b7778bed4a27aa8482e1eecc116)\n",
    "\n",
    "\n",
    "2. Extract trackable features \n",
    "\n",
    "    Shi-Tomasi Corner Detector is an implementation in openCV which extracts information from the ROI input. The extracted information are points on the face which are are trackable across a sequence of moving frames (a video).\n",
    "\n",
    "    **References** : \n",
    "    [OpenCV Trackable feature extraction function(Shi-Tomasi Corner Detector)](https://docs.opencv.org/4.5.2/d4/d8c/tutorial_py_shi_tomasi.html)\n",
    "\n",
    "\n",
    "3. Calculate the optical flow\n",
    "\n",
    "    These trackable points are used to calculate the optical flow of the faces with calcOpticalFlowPyrLK() function. The tracking is visualized via OpenCV drawing tools.\n",
    "\n",
    "    **References** : \n",
    "    - [Optical Flow calculation](https://docs.opencv.org/4.5.3/d4/dee/tutorial_optical_flow.html)\n",
    "    - [OpenCV drawing functions](https://docs.opencv.org/4.5.2/dc/da5/tutorial_py_drawing_functions.html)\n",
    "\n",
    "**Your task**\n",
    "\n",
    "Complete the function which perfoms following\n",
    "\n",
    "- [x] Takes grayscale image and resized image as the input\n",
    "- [x] Detect faces in graycale image using cascade classifier. detectMultiscale() function returns detected faces as rectangles ( Top left x coordinate, Top left y coordinate, width, height)\n",
    "- [ ] Draw a rectangle around detected faces using OpenCV drawing functions\n",
    "- [ ] Slice a region of interest (ROI) from grayecale image corresponding to the detections\n",
    "- [x] Extract good features to track (p0), from OpenCV goodFeaturesToTrack() function.\n",
    "- [ ] Convert the array p0 from current format [[[x1,y1],[x2,y2],....]] to --> [[x1,y1],[x2,y2],....]. Tip : print p0 to observe current format\n",
    "- [ ] The points are located with respect to the ROI coordinates. Convert them to image coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d3e53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[299.0, 152.0],\n",
       "  [259.0, 152.0],\n",
       "  [301.0, 158.0],\n",
       "  [234.0, 116.0],\n",
       "  [299.0, 146.0],\n",
       "  [325.0, 135.0],\n",
       "  [306.0, 116.0],\n",
       "  [295.0, 155.0],\n",
       "  [271.0, 157.0],\n",
       "  [274.0, 188.0],\n",
       "  [277.0, 207.0],\n",
       "  [309.0, 125.0],\n",
       "  [306.0, 121.0],\n",
       "  [278.0, 192.0],\n",
       "  [306.0, 136.0],\n",
       "  [285.0, 188.0],\n",
       "  [241.0, 119.0],\n",
       "  [297.0, 161.0],\n",
       "  [278.0, 199.0],\n",
       "  [330.0, 144.0],\n",
       "  [327.0, 126.0],\n",
       "  [314.0, 126.0],\n",
       "  [288.0, 145.0],\n",
       "  [303.0, 204.0],\n",
       "  [255.0, 140.0],\n",
       "  [258.0, 157.0],\n",
       "  [309.0, 158.0],\n",
       "  [271.0, 200.0],\n",
       "  [331.0, 123.0],\n",
       "  [305.0, 142.0],\n",
       "  [330.0, 132.0],\n",
       "  [301.0, 119.0],\n",
       "  [263.0, 147.0],\n",
       "  [317.0, 154.0],\n",
       "  [319.0, 148.0],\n",
       "  [287.0, 207.0],\n",
       "  [269.0, 150.0],\n",
       "  [250.0, 145.0],\n",
       "  [310.0, 147.0],\n",
       "  [313.0, 121.0],\n",
       "  [329.0, 163.0],\n",
       "  [266.0, 161.0],\n",
       "  [291.0, 159.0],\n",
       "  [318.0, 140.0],\n",
       "  [274.0, 181.0],\n",
       "  [321.0, 131.0],\n",
       "  [252.0, 119.0],\n",
       "  [325.0, 143.0],\n",
       "  [334.0, 136.0],\n",
       "  [304.0, 128.0],\n",
       "  [330.0, 153.0],\n",
       "  [230.0, 201.0],\n",
       "  [315.0, 134.0],\n",
       "  [252.0, 160.0],\n",
       "  [258.0, 144.0],\n",
       "  [334.0, 161.0],\n",
       "  [259.0, 120.0],\n",
       "  [274.0, 220.0],\n",
       "  [285.0, 155.0],\n",
       "  [271.0, 214.0],\n",
       "  [326.0, 159.0],\n",
       "  [311.0, 130.0],\n",
       "  [242.0, 126.0],\n",
       "  [289.0, 182.0],\n",
       "  [318.0, 159.0],\n",
       "  [293.0, 127.0],\n",
       "  [321.0, 119.0],\n",
       "  [299.0, 207.0],\n",
       "  [247.0, 123.0],\n",
       "  [271.0, 162.0]],\n",
       " array([[229, 115, 107, 107]], dtype=int32),\n",
       " array([[[ 62,  63,  73],\n",
       "         [ 59,  60,  70],\n",
       "         [ 56,  57,  67],\n",
       "         ...,\n",
       "         [ 60,  76, 112],\n",
       "         [ 61,  77, 113],\n",
       "         [ 61,  77, 113]],\n",
       " \n",
       "        [[ 59,  60,  70],\n",
       "         [ 56,  57,  67],\n",
       "         [ 54,  55,  65],\n",
       "         ...,\n",
       "         [ 60,  76, 112],\n",
       "         [ 60,  76, 112],\n",
       "         [ 61,  77, 113]],\n",
       " \n",
       "        [[ 56,  57,  67],\n",
       "         [ 54,  55,  65],\n",
       "         [ 52,  53,  63],\n",
       "         ...,\n",
       "         [ 60,  76, 112],\n",
       "         [ 60,  76, 112],\n",
       "         [ 61,  77, 113]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 73,  85, 127],\n",
       "         [ 71,  83, 125],\n",
       "         [ 69,  81, 123],\n",
       "         ...,\n",
       "         [ 57,  78, 130],\n",
       "         [ 57,  77, 129],\n",
       "         [ 58,  76, 129]],\n",
       " \n",
       "        [[ 73,  87, 132],\n",
       "         [ 71,  85, 130],\n",
       "         [ 69,  83, 128],\n",
       "         ...,\n",
       "         [ 59,  78, 129],\n",
       "         [ 60,  77, 130],\n",
       "         [ 61,  77, 130]],\n",
       " \n",
       "        [[ 74,  89, 138],\n",
       "         [ 72,  87, 136],\n",
       "         [ 70,  85, 134],\n",
       "         ...,\n",
       "         [ 61,  78, 129],\n",
       "         [ 63,  78, 130],\n",
       "         [ 64,  78, 131]]], dtype=uint8))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_trackable_points(gray,img):\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "    p0=[]\n",
    "    if len(faces) != 0:\n",
    "        ## ToDO 4.1.3\n",
    "        # for (x,y,w,h) in faces:\n",
    "        #   draw rectang\n",
    "        #   slice ROI      \n",
    "        rois = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            roi_gray=gray[y:y+h,x:x+w]\n",
    "            rois.append(roi_gray)\n",
    "            # Extract good features to track (p0) from each ROI\n",
    "            p0_roi = cv2.goodFeaturesToTrack(roi_gray, maxCorners=70, qualityLevel=0.001, minDistance=5)\n",
    "            p0.append(p0_roi)\n",
    "        \n",
    "        # Convert the array p0 from the format [[[x1, y1]], [[x2, y2]], ...] to [[x1, y1], [x2, y2], ...]\n",
    "        #p0 = [item for sublist in p0 for item in sublist]\n",
    "        p0 = [item[0] for sublist in p0 for item in sublist]\n",
    "\n",
    "        # Convert the points from ROI coordinates to image coordinates\n",
    "        p0 = [[x + x1, y + y1] for (x1, y1) in p0]\n",
    "   \n",
    "    return p0, faces, img\n",
    "\n",
    "img = cv2.imread('len_top.jpg')\n",
    "gray, img = prep(img)\n",
    "get_trackable_points(gray, img)\n",
    "#print('output:',get_trackable_points(gray, img))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6c1c4",
   "metadata": {},
   "source": [
    "**Your task**\n",
    "\n",
    "Complete the do_track_face() function which perfoms following\n",
    "\n",
    "- [x] Usecv2.calcOpticalFlowPyrLK()to calculate the optical flow for tracking face\n",
    "- [ ] Select the valid points from p1. Note that  isFound == 1 for valid points \n",
    "- [ ] Return the valid points as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6754539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_track_face(gray_prev, gray, p0):\n",
    "    gray_prev_umat = cv2.UMat(gray_prev)\n",
    "    gray_umat = cv2.UMat(gray)\n",
    "    p0_umat = cv2.UMat(np.float32(np.array(p0)))\n",
    "    \n",
    "    p0_umat=np.float32(p0)\n",
    " \n",
    "    p1, isFound, err = cv2.calcOpticalFlowPyrLK(gray_prev, gray, p0_umat, \n",
    "                                                            None,\n",
    "                                                            winSize=(31,31),\n",
    "                                                            maxLevel=10,\n",
    "                                                            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.03),\n",
    "                                                            flags=cv2.OPTFLOW_LK_GET_MIN_EIGENVALS,\n",
    "                                                            minEigThreshold=0.00025)\n",
    "    isFound = np.squeeze(isFound)\n",
    "    ## ToDo 4.1.3 - Select valid points from p1\n",
    "    valid_points = p1[isFound==1]\n",
    "\n",
    "    # return a numpy array of selected points from p1\n",
    "    return np.array(valid_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740edad5",
   "metadata": {},
   "source": [
    "### 4.1.4\n",
    "\n",
    "Run the program to view the final output of face tracking. Remember to enter the correct path to video file provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "844eda03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5905/3444691046.py:35: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  cv2.drawMarker(img, (i[0], i[1]),[255,0,0],0)\n"
     ]
    }
   ],
   "source": [
    "frame_rate = 30\n",
    "prev = 0\n",
    "gray_prev = None\n",
    "p0 = []\n",
    "#cam = cv2.VideoCapture(\"Face.mp4\")\n",
    "cam = cv2.VideoCapture(0)\n",
    "if not cam.isOpened():\n",
    "    raise Exception(\"Could not open camera/file\")\n",
    "    \n",
    "while True:\n",
    "    time_elapsed = time.time() - prev\n",
    "    \n",
    "    if time_elapsed > 1./frame_rate:\n",
    "        \n",
    "        ret_val,img = cam.read()\n",
    "        \n",
    "        if not ret_val:\n",
    "                cam.set(cv2.CAP_PROP_POS_FRAMES, 0)  # restart video\n",
    "                gray_prev = None  # previous frame\n",
    "                p0 = []  # previous points\n",
    "                continue\n",
    "        prev = time.time()\n",
    "        \n",
    "        gray, img = prep(img)\n",
    "\n",
    "        if len(p0) <= 10: \n",
    "            p0, faces, img = get_trackable_points(gray,img)\n",
    "            gray_prev = gray.copy()\n",
    "        \n",
    "        else:\n",
    "            for (x,y,w,h) in faces:\n",
    "                cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "                p1 = do_track_face(gray_prev, gray, p0)\n",
    "            for i in p1:\n",
    "                cv2.drawMarker(img, (i[0], i[1]),[255,0,0],0)\n",
    "            p0 = p1\n",
    "                   \n",
    "        cv2.imshow('Video feed', img)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "              \n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7156018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee6acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d79c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracker",
   "language": "python",
   "name": "tracker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
